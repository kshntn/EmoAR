{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conversion PyTorch to Caffe with tool.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WlAXUalbmW2",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/marvis/pytorch-caffe-darknet-convert/blob/master/pytorch2caffe.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBKrAJcEIvB",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUTU665fhTcx",
        "colab_type": "code",
        "outputId": "cc1f591e-f273-4761-f9a9-5eeb3886c11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_2kGGrviDfU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUBsJZ3vhI51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install onnx\n",
        "\n",
        "help(torch.onnx.export)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgw2hbFPhhNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade --force-reinstall torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zme3X9MtCW0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall torchvision\n",
        "#!pip install -c pytorch pytorch-nightly torchvision cudatoolkit=10.0\n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJU3-scJq6_L",
        "colab_type": "code",
        "outputId": "23c5042a-483c-40f2-d6b5-a388b717004f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install caffe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting caffe\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement caffe (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for caffe\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgJQHdjih4jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some standard imports\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx\n",
        "import torchvision\n",
        "\n",
        "# Super Resolution model definition in PyTorch\n",
        "import torch.nn.init as init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BOgbKKTqqyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "#sys.path.append('/data/temp/caffe/python')\n",
        "import caffe\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from prototxt import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fTGvFLCUtxI",
        "colab_type": "code",
        "outputId": "7d6fd957-4c2e-421c-9ac9-37fd1755206e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n",
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoyz_BGNEGEo",
        "colab_type": "text"
      },
      "source": [
        "### Define the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv5234YBEFTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = './gdrive/My Drive/Colab Notebooks/Fer-dataset/' \n",
        "checkpoint_name = 'akash-mobilenet_v2-FER1-60perc.pt'\n",
        "onnx_export_path = base_path + \"ONNX/akash_mobilenet_60_bc2.onnx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6Ct8SsD-Fg",
        "colab_type": "text"
      },
      "source": [
        "To export a model, you call the torch.onnx._export() function. \n",
        "This will execute the model, recording a trace of what operators are used to compute the outputs. \n",
        "Because _export runs the model, we need provide an input tensor x. \n",
        "The values in this tensor are not important; it can be an image or a \n",
        "random tensor as long as it is the right size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7EoGmah_PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Standard ImageNet input - 3 channels, 224x224,\n",
        "# values don't matter as we care about network structure.\n",
        "# But they can also be real inputs.\n",
        "\n",
        "\n",
        "# A model class instance (class not shown)\n",
        "torch_model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "torch_model.classifier[1] = torch.nn.Linear(1280, 7)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# Initialize model with the pretrained weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "\n",
        "# Load the weights from a file (.pth usually)\n",
        "state_dict = torch.load(base_path + checkpoint_name, map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the weights now into a model net architecture defined by our class\n",
        "torch_model.load_state_dict(state_dict)\n",
        "\n",
        "# set the train mode to false since we will only run the forward pass.\n",
        "torch_model.train(False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO9kXe5iQ4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the right input shape (e.g. for an image)\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "\n",
        "torch.onnx.export(torch_model, dummy_input, onnx_export_path)\n",
        "\n",
        "# Export the model\n",
        "torch_out = torch.onnx._export(torch_model,             # model being run\n",
        "                               dummy_input,             # model input (or a tuple for multiple inputs)\n",
        "                               onnx_export_path,             # where to save the model (can be a file or file-like object)\n",
        "                               export_params=True)      # store the trained parameter weights inside the model file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MxPrnCMEcvG",
        "colab_type": "text"
      },
      "source": [
        "torch_out is the output after executing the model. Normally you can ignore this output, but here we will use it to verify that the model we exported computes the same values when run in Caffe2.\n",
        "\n",
        "Now letâ€™s take the ONNX representation and use it in Caffe2. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that Caffe2 and PyTorch are computing the same value for the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyjBZggzqpP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b-Mq4iyoLHQ",
        "colab_type": "code",
        "outputId": "1b73b532-ee3d-455b-eab4-4cec55558406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "layer_dict = {'ConvNdBackward'    : 'Convolution',\n",
        "              'ThresholdBackward' : 'ReLU',\n",
        "              'MaxPool2dBackward' : 'Pooling',\n",
        "              'AvgPool2dBackward' : 'Pooling',\n",
        "              'DropoutBackward'   : 'Dropout',\n",
        "              'AddmmBackward'     : 'InnerProduct',\n",
        "              'BatchNormBackward' : 'BatchNorm',\n",
        "              'AddBackward'       : 'Eltwise',\n",
        "              'SoftmaxBackward'   : 'Softmax',\n",
        "              'ViewBackward'      : 'Reshape'}\n",
        "\n",
        "layer_id = 0\n",
        "def pytorch2caffe(input_var, output_var, protofile, caffemodel):\n",
        "    global layer_id\n",
        "    net_info = pytorch2prototxt(input_var, output_var)\n",
        "    print_prototxt(net_info)\n",
        "    save_prototxt(net_info, protofile)\n",
        "\n",
        "    net = caffe.Net(protofile, caffe.TEST)\n",
        "    params = net.params\n",
        "\n",
        "    layer_id = 1\n",
        "    seen = set()\n",
        "    def convert_layer(func):\n",
        "        if True:\n",
        "            global layer_id\n",
        "            parent_type = str(type(func).__name__)\n",
        "    \n",
        "            if hasattr(func, 'next_functions'):\n",
        "                for u in func.next_functions:\n",
        "                    if u[0] is not None:\n",
        "                        child_type = str(type(u[0]).__name__)\n",
        "                        child_name = child_type + str(layer_id)\n",
        "                        if child_type != 'AccumulateGrad' and (parent_type != 'AddmmBackward' or child_type != 'TransposeBackward'):\n",
        "                            if u[0] not in seen:\n",
        "                                convert_layer(u[0])\n",
        "                                seen.add(u[0])\n",
        "                            if child_type != 'ViewBackward':\n",
        "                                layer_id = layer_id + 1\n",
        "    \n",
        "            parent_name = parent_type+str(layer_id)\n",
        "            print('converting %s' % parent_name)\n",
        "            if parent_type == 'ConvNdBackward':\n",
        "                weights = func.next_functions[1][0].variable.data\n",
        "                if func.next_functions[2][0]:\n",
        "                    biases = func.next_functions[2][0].variable.data\n",
        "                else:\n",
        "                    biases = None\n",
        "                save_conv2caffe(weights, biases, params[parent_name])\n",
        "            elif parent_type == 'BatchNormBackward':\n",
        "                running_mean = func.running_mean\n",
        "                running_var = func.running_var\n",
        "                #print('%s running_mean' % parent_name, running_mean)\n",
        "                #exit(0)\n",
        "                scale_weights = func.next_functions[1][0].variable.data\n",
        "                scale_biases = func.next_functions[2][0].variable.data\n",
        "                bn_name = parent_name + \"_bn\"\n",
        "                scale_name = parent_name + \"_scale\"\n",
        "                save_bn2caffe(running_mean, running_var, params[bn_name])\n",
        "                save_scale2caffe(scale_weights, scale_biases, params[scale_name])\n",
        "            elif parent_type == 'AddmmBackward':\n",
        "                biases = func.next_functions[0][0].variable.data\n",
        "                weights = func.next_functions[2][0].next_functions[0][0].variable.data\n",
        "                save_fc2caffe(weights, biases, params[parent_name])\n",
        "        \n",
        "    convert_layer(output_var.grad_fn)\n",
        "    print('save caffemodel to %s' % caffemodel)\n",
        "    net.save(caffemodel)\n",
        "\n",
        "def save_conv2caffe(weights, biases, conv_param):\n",
        "    if biases is not None:\n",
        "        conv_param[1].data[...] = biases.numpy() \n",
        "    conv_param[0].data[...] = weights.numpy() \n",
        "\n",
        "def save_fc2caffe(weights, biases, fc_param):\n",
        "    fc_param[1].data[...] = biases.numpy() \n",
        "    fc_param[0].data[...] = weights.numpy() \n",
        "\n",
        "def save_bn2caffe(running_mean, running_var, bn_param):\n",
        "    bn_param[0].data[...] = running_mean.numpy()\n",
        "    bn_param[1].data[...] = running_var.numpy()\n",
        "    bn_param[2].data[...] = np.array([1.0])\n",
        "\n",
        "def save_scale2caffe(weights, biases, scale_param):\n",
        "    scale_param[1].data[...] = biases.numpy()\n",
        "    scale_param[0].data[...] = weights.numpy()\n",
        "\n",
        "#def pytorch2prototxt(model, x, var):\n",
        "def pytorch2prototxt(input_var, output_var):\n",
        "    global layer_id\n",
        "    net_info = OrderedDict()\n",
        "    props = OrderedDict()\n",
        "    props['name'] = 'pytorch'\n",
        "    props['input'] = 'data'\n",
        "    props['input_dim'] = input_var.size()\n",
        "    \n",
        "    layers = []\n",
        "\n",
        "    layer_id = 1\n",
        "    seen = set()\n",
        "    top_names = dict()\n",
        "    def add_layer(func):\n",
        "        global layer_id\n",
        "        parent_type = str(type(func).__name__)\n",
        "        parent_bottoms = []\n",
        "\n",
        "        if hasattr(func, 'next_functions'):\n",
        "            for u in func.next_functions:\n",
        "                if u[0] is not None:\n",
        "                    child_type = str(type(u[0]).__name__)\n",
        "                    child_name = child_type + str(layer_id)\n",
        "                    if child_type != 'AccumulateGrad' and (parent_type != 'AddmmBackward' or child_type != 'TransposeBackward'):\n",
        "                        if u[0] not in seen:\n",
        "                            top_name = add_layer(u[0])\n",
        "                            parent_bottoms.append(top_name)\n",
        "                            seen.add(u[0])\n",
        "                        else:\n",
        "                            top_name = top_names[u[0]]\n",
        "                            parent_bottoms.append(top_name)\n",
        "                        if child_type != 'ViewBackward':\n",
        "                            layer_id = layer_id + 1\n",
        "    \n",
        "        parent_name = parent_type+str(layer_id)\n",
        "        layer = OrderedDict()\n",
        "        layer['name'] = parent_name\n",
        "        layer['type'] = layer_dict[parent_type]\n",
        "        parent_top = parent_name\n",
        "        if len(parent_bottoms) > 0:\n",
        "            layer['bottom'] = parent_bottoms \n",
        "        else:\n",
        "            layer['bottom'] = ['data']\n",
        "        layer['top'] = parent_top\n",
        "        if parent_type == 'ConvNdBackward':\n",
        "            weights = func.next_functions[1][0].variable\n",
        "            conv_param = OrderedDict()\n",
        "            conv_param['num_output'] = weights.size(0)\n",
        "            conv_param['pad'] = func.padding[0]\n",
        "            conv_param['kernel_size'] = weights.size(2)\n",
        "            conv_param['stride'] = func.stride[0]\n",
        "            if func.next_functions[2][0] == None:\n",
        "                conv_param['bias_term'] = 'false'\n",
        "            layer['convolution_param'] = conv_param\n",
        "        elif parent_type == 'BatchNormBackward':\n",
        "            bn_layer = OrderedDict()\n",
        "            bn_layer['name'] = parent_name + \"_bn\"\n",
        "            bn_layer['type'] = 'BatchNorm'\n",
        "            bn_layer['bottom'] = parent_bottoms\n",
        "            bn_layer['top'] = parent_top\n",
        "            batch_norm_param = OrderedDict()\n",
        "            batch_norm_param['use_global_stats'] = 'true'\n",
        "            bn_layer['batch_norm_param'] = batch_norm_param\n",
        "    \n",
        "            scale_layer = OrderedDict()\n",
        "            scale_layer['name'] = parent_name + \"_scale\"\n",
        "            scale_layer['type'] = 'Scale'\n",
        "            scale_layer['bottom'] = parent_top\n",
        "            scale_layer['top'] = parent_top\n",
        "            scale_param = OrderedDict()\n",
        "            scale_param['bias_term'] = 'true'\n",
        "            scale_layer['scale_param'] = scale_param\n",
        "        elif parent_type == 'ThresholdBackward':\n",
        "            parent_top = parent_bottoms[0]\n",
        "        elif parent_type == 'SoftmaxBackward':\n",
        "            parent_top = parent_bottoms[0]\n",
        "        elif parent_type == 'MaxPool2dBackward':\n",
        "            pooling_param = OrderedDict()\n",
        "            pooling_param['pool'] = 'MAX'\n",
        "            pooling_param['kernel_size'] = func.kernel_size[0]\n",
        "            pooling_param['stride'] = func.stride[0]\n",
        "            pooling_param['pad'] = func.padding[0]\n",
        "            layer['pooling_param']  = pooling_param\n",
        "        elif parent_type == 'AvgPool2dBackward':\n",
        "            pooling_param = OrderedDict()\n",
        "            pooling_param['pool'] = 'AVE'\n",
        "            pooling_param['kernel_size'] = func.kernel_size[0]\n",
        "            pooling_param['stride'] = func.stride[0]\n",
        "            layer['pooling_param'] = pooling_param\n",
        "        elif parent_type == 'DropoutBackward':\n",
        "            parent_top = parent_bottoms[0]\n",
        "            dropout_param = OrderedDict()\n",
        "            dropout_param['dropout_ratio'] = func.p\n",
        "            layer['dropout_param'] = dropout_param\n",
        "        elif parent_type == 'AddmmBackward':\n",
        "            inner_product_param = OrderedDict()\n",
        "            inner_product_param['num_output'] = func.next_functions[0][0].variable.size(0)\n",
        "            layer['inner_product_param'] = inner_product_param\n",
        "        elif parent_type == 'ViewBackward':\n",
        "            parent_top = parent_bottoms[0]\n",
        "        elif parent_type == 'AddBackward':\n",
        "            eltwise_param = OrderedDict()\n",
        "            eltwise_param['operation'] = 'SUM'\n",
        "            layer['eltwise_param'] = eltwise_param\n",
        "    \n",
        "        layer['top'] = parent_top # reset layer['top'] as parent_top may change\n",
        "        if parent_type != 'ViewBackward':\n",
        "            if parent_type == \"BatchNormBackward\":\n",
        "                layers.append(bn_layer)\n",
        "                layers.append(scale_layer)\n",
        "            else:\n",
        "                layers.append(layer)\n",
        "            #layer_id = layer_id + 1\n",
        "        top_names[func] = parent_top\n",
        "        return parent_top\n",
        "    \n",
        "    add_layer(output_var.grad_fn)\n",
        "    net_info['props'] = props\n",
        "    net_info['layers'] = layers\n",
        "    return net_info\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import torchvision\n",
        "    from visualize import make_dot\n",
        "\n",
        "    model_name = 'resnet50'\n",
        "\n",
        "    if model_name == 'resnet50':\n",
        "        m = torchvision.models.resnet50(pretrained=True)\n",
        "    elif model_name == 'vgg16':\n",
        "        m = torchvision.models.vgg16()\n",
        "        m.classifier.add_module('softmax', torch.nn.Softmax())\n",
        "    m.eval() # very important here, otherwise batchnorm running_mean, running_var will be incorrect\n",
        "    input_var = Variable(torch.rand(1, 3, 224, 224))\n",
        "\n",
        "    print(m)\n",
        "    output_var = m(input_var)\n",
        "    fp = open(\"out.dot\", \"w\")\n",
        "    dot = make_dot(output_var)\n",
        "    print >> fp, dot\n",
        "    fp.close()\n",
        "    #exit(0)\n",
        "\n",
        "    if model_name == 'resnet50':\n",
        "        pytorch2caffe(input_var, output_var, 'resnet50-pytorch2caffe.prototxt', 'resnet50-pytorch2caffe.caffemodel')\n",
        "    elif model_name == 'vgg16':\n",
        "        pytorch2caffe(input_var, output_var, 'vgg16-pytorch2caffe.prototxt', 'vgg16-pytorch2caffe.caffemodel')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<google.protobuf.pyext._message.MessageDescriptor object at 0x7f10c1c60db0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2LFundiwqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}