{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conversion PyTorch to Caffe2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "XGBKrAJcEIvB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WlAXUalbmW2",
        "colab_type": "text"
      },
      "source": [
        "https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html\n",
        "\n",
        "https://blog.exxactcorp.com/pytorch-release-v1-2-0-new-torchscript-api-with-improved-python-language-coverage-expanded-onnx-export-nn-transformer/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBKrAJcEIvB",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUTU665fhTcx",
        "colab_type": "code",
        "outputId": "b07d1b7a-4e32-497f-e444-7a90f7eb373a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_2kGGrviDfU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUBsJZ3vhI51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install onnx\n",
        "\n",
        "help(torch.onnx.export)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgw2hbFPhhNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade --force-reinstall torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zme3X9MtCW0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall torchvision\n",
        "#!pip install -c pytorch pytorch-nightly torchvision cudatoolkit=10.0\n",
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgJQHdjih4jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some standard imports\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.onnx\n",
        "import torchvision\n",
        "\n",
        "# Super Resolution model definition in PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fTGvFLCUtxI",
        "colab_type": "code",
        "outputId": "7d6fd957-4c2e-421c-9ac9-37fd1755206e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2.0\n",
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoyz_BGNEGEo",
        "colab_type": "text"
      },
      "source": [
        "### Define the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv5234YBEFTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = './gdrive/My Drive/Colab Notebooks/Fer-dataset/' \n",
        "checkpoint_name = 'akash-mobilenet_v2-FER1-60perc.pt'\n",
        "onnx_export_path = base_path + \"ONNX/akash_mobilenet_60_bc2.onnx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6Ct8SsD-Fg",
        "colab_type": "text"
      },
      "source": [
        "To export a model, you call the torch.onnx._export() function. \n",
        "This will execute the model, recording a trace of what operators are used to compute the outputs. \n",
        "Because _export runs the model, we need provide an input tensor x. \n",
        "The values in this tensor are not important; it can be an image or a \n",
        "random tensor as long as it is the right size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7EoGmah_PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Standard ImageNet input - 3 channels, 224x224,\n",
        "# values don't matter as we care about network structure.\n",
        "# But they can also be real inputs.\n",
        "\n",
        "\n",
        "# A model class instance (class not shown)\n",
        "torch_model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "torch_model.classifier[1] = torch.nn.Linear(1280, 7)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# Initialize model with the pretrained weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "\n",
        "# Load the weights from a file (.pth usually)\n",
        "state_dict = torch.load(base_path + checkpoint_name, map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the weights now into a model net architecture defined by our class\n",
        "torch_model.load_state_dict(state_dict)\n",
        "\n",
        "# set the train mode to false since we will only run the forward pass.\n",
        "torch_model.train(False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO9kXe5iQ4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the right input shape (e.g. for an image)\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "\n",
        "torch.onnx.export(torch_model, dummy_input, onnx_export_path)\n",
        "\n",
        "# Export the model\n",
        "torch_out = torch.onnx._export(torch_model,             # model being run\n",
        "                               dummy_input,             # model input (or a tuple for multiple inputs)\n",
        "                               onnx_export_path,             # where to save the model (can be a file or file-like object)\n",
        "                               export_params=True)      # store the trained parameter weights inside the model file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MxPrnCMEcvG",
        "colab_type": "text"
      },
      "source": [
        "torch_out is the output after executing the model. Normally you can ignore this output, but here we will use it to verify that the model we exported computes the same values when run in Caffe2.\n",
        "\n",
        "Now let’s take the ONNX representation and use it in Caffe2. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that Caffe2 and PyTorch are computing the same value for the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG8f0a7YEdtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import onnx\n",
        "import caffe2.python.onnx.backend as onnx_caffe2_backend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8VKht6mEnqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the ONNX ModelProto object. model is a standard Python protobuf object\n",
        "model = onnx.load(onnx_export_path)\n",
        "\n",
        "# prepare the caffe2 backend for executing the model this converts the ONNX model into a\n",
        "# Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be\n",
        "# availiable soon.\n",
        "prepared_backend = onnx_caffe2_backend.prepare(model)\n",
        "\n",
        "# run the model in Caffe2\n",
        "\n",
        "# Construct a map from input names to Tensor data.\n",
        "# The graph of the model itself contains inputs for all weight parameters, after the input image.\n",
        "# Since the weights are already embedded, we just need to pass the input image.\n",
        "# Set the first input.\n",
        "W = {model.graph.input[0].name: dummy_input.data.numpy()}\n",
        "\n",
        "print(model.graph.input[0])\n",
        "print(model.graph.output[0])\n",
        "\n",
        "# Run the Caffe2 net:\n",
        "c2_out = prepared_backend.run(W)[0]\n",
        "\n",
        "# Verify the numerical correctness upto 3 decimal places\n",
        "np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)\n",
        "\n",
        "print(\"Exported model has been executed on Caffe2 backend, and the result looks good!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5u_Vit7HH3I",
        "colab_type": "code",
        "outputId": "564aa575-7189-4225-dab2-5c4cb16ff56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prepared_backend # caffe2.python.onnx.backend_rep.Caffe2Rep"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<caffe2.python.onnx.backend_rep.Caffe2Rep at 0x7fa3d2399048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUhxxlvuHaqW",
        "colab_type": "text"
      },
      "source": [
        "Running the model on mobile devices\n",
        "So far we have exported a model from PyTorch and shown how to load it and run it in Caffe2. Now that the model is loaded in Caffe2, we can convert it into a format suitable for running on mobile devices.\n",
        "\n",
        "We will use Caffe2’s mobile_exporter to generate the two model protobufs that can run on mobile. The first is used to initialize the network with the correct weights, and the second actual runs executes the model. We will continue to use the small super-resolution model for the rest of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX5ijZkdHbxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the workspace and the model proto from the internal representation\n",
        "c2_workspace = prepared_backend.workspace\n",
        "c2_model = prepared_backend.predict_net\n",
        "\n",
        "# Now import the caffe2 mobile exporter\n",
        "from caffe2.python.predictor import mobile_exporter\n",
        "\n",
        "# call the Export to get the predict_net, init_net. These nets are needed for running things on mobile\n",
        "init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)\n",
        "\n",
        "# Let's also save the init_net and predict_net to a file that we will later use for running them on mobile\n",
        "with open(base_path +'ONNX/akash_mobilenet_init_net.pb', \"wb\") as fopen:\n",
        "    fopen.write(init_net.SerializeToString())\n",
        "with open(base_path + 'ONNX/akash_mobilenet_predict_net.pb', \"wb\") as fopen:\n",
        "    fopen.write(predict_net.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12trM1rXIADT",
        "colab_type": "text"
      },
      "source": [
        "init_net has the model parameters and the model input embedded in it and predict_net will be used to guide the init_net execution at run-time. In this tutorial, we will use the init_net and predict_net generated above and run them in both normal Caffe2 backend and mobile and verify that the output high-resolution cat image produced in both runs is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-BTrfphIBIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some standard imports\n",
        "from caffe2.proto import caffe2_pb2\n",
        "from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot\n",
        "from skimage import io, transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gRNJmR_IO6Y",
        "colab_type": "text"
      },
      "source": [
        "load the image, pre-process it using standard skimage python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yUYygqTIOFS",
        "colab_type": "code",
        "outputId": "cd6c5c94-9a83-4a98-e212-ce1559dadb5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# path to test image\n",
        "path_to_Test_input_img = base_path + \"testface-happy.jpg\"\n",
        "path_to_Test_output_img = base_path + \"testface_224x224.jpg\"\n",
        "\n",
        "\n",
        "# load the image\n",
        "img_in = io.imread(path_to_Test_input_img)\n",
        "\n",
        "# resize the image to dimensions 224x224\n",
        "img = transform.resize(img_in, [256, 256])\n",
        "\n",
        "\n",
        "# save this resized image to be used as input to the model\n",
        "io.imsave(path_to_Test_output_img, img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEYdOOFOJdqb",
        "colab_type": "text"
      },
      "source": [
        "Now, as a next step, let’s take the resized  image and run the  model in Caffe2 backend and save the output image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7Kv6RNIJcFS",
        "colab_type": "code",
        "outputId": "d6e90278-b6d2-48bf-e267-8a2e08bf8aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load the resized image and convert it to Ybr format\n",
        "#img = Image.open(path_to_Test_output_img)\n",
        "#img_ycbcr = img.convert('YCbCr')\n",
        "#img_y, img_cb, img_cr = img_ycbcr.split()\n",
        "\n",
        "#print(img.size)\n",
        "\n",
        "from matplotlib.image import imread\n",
        "\n",
        "img_np = imread(path_to_Test_output_img)\n",
        "#print(type(img_np))\n",
        "#print(img_np.shape)\n",
        "img_np = img_np.T\n",
        "\n",
        "\n",
        "# Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized\n",
        "workspace.RunNetOnce(init_net)\n",
        "workspace.RunNetOnce(predict_net)\n",
        "\n",
        "# Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify\n",
        "# what our input and output blob names are.\n",
        "\n",
        "#print(net_printer.to_string(predict_net))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2TEFtYZKPhy",
        "colab_type": "text"
      },
      "source": [
        "From the above output, we can see that input is named “input.1” and output is named “465”(it is a little bit weird that we will have numbers as blob names but this is because the tracing JIT produces numbered entries for the models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQRaq4BWKflV",
        "colab_type": "code",
        "outputId": "46753837-7b5e-401c-88e3-841c761eaec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Now, let's also pass in the resized cat image for processing by the model.\n",
        "#workspace.FeedBlob(\"input.1\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n",
        "\n",
        "pass_img = np.broadcast_to(img_np, (1, 3, 256, 256)) \n",
        "\n",
        "print (pass_img.shape)\n",
        "\n",
        "workspace.FeedBlob(\"input.1\", np.array(pass_img).astype(np.float32))\n",
        "\n",
        "# run the predict_net to get the model output\n",
        "workspace.RunNetOnce(predict_net)\n",
        "\n",
        "# Now let's get the model output blob\n",
        "prediction = workspace.FetchBlob(\"465\")\n",
        "\n",
        "print(prediction)\n",
        "# (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n",
        "print(np.max(prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 256, 256)\n",
            "[[ 3.5308531e-01 -3.3968492e+00  2.0000190e-03 -6.9760841e-01\n",
            "   6.6572469e-01  2.2781390e-01  4.6561572e-01]]\n",
            "0.6657247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b-Mq4iyoLHQ",
        "colab_type": "code",
        "outputId": "1b73b532-ee3d-455b-eab4-4cec55558406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(init_net.DESCRIPTOR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<google.protobuf.pyext._message.MessageDescriptor object at 0x7f10c1c60db0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2LFundiwqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}